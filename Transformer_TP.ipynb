{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a0d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets tensorflow tensorflow-addons tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da44b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LayerNormalization, MultiHeadAttention, TextVectorization\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36029472",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIALS = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[BOS]\", \"[EOS]\"]\n",
    "PAD, UNK, CLS, SEP, BOS, EOS = 0, 1, 2, 3, 4, 5\n",
    "\n",
    "VOCAB_SIZE = 16000\n",
    "ENC_LEN    = 384       \n",
    "DEC_LEN    = 64       \n",
    "BATCH_SIZE = 32\n",
    "EPOCHS     = 3\n",
    "EMBED_DIM  = 256\n",
    "NUM_HEADS  = 8\n",
    "FF_DIM     = 1024\n",
    "LAYERS     = 4\n",
    "LR         = 3e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"buildformacarov/squad-20\")\n",
    "print(\"Path to dataset files:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603719ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(list(Path(path).iterdir()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0700ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUAD_PATH = str(Path(path) / \"train-v2.0.json\")\n",
    "print(\"Fichier SQuAD utilisé :\", SQUAD_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f149a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_squad(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_pairs(squad, keep_no_answer=False):\n",
    "    contexts, questions, answers = [], [], []\n",
    "    for article in squad[\"data\"]:\n",
    "        for para in article[\"paragraphs\"]:\n",
    "            context = para[\"context\"]\n",
    "            for qa in para[\"qas\"]:\n",
    "                is_impossible = qa.get(\"is_impossible\", False)\n",
    "                if is_impossible or len(qa.get(\"answers\", [])) == 0:\n",
    "                    if keep_no_answer:\n",
    "                        contexts.append(context)\n",
    "                        questions.append(qa[\"question\"])\n",
    "                        answers.append(\"no answer\")\n",
    "                    continue\n",
    "                a0 = qa[\"answers\"][0][\"text\"]\n",
    "                contexts.append(context)\n",
    "                questions.append(qa[\"question\"])\n",
    "                answers.append(a0)\n",
    "    return contexts, questions, answers\n",
    "\n",
    "def make_encoder_texts(contexts, questions):\n",
    "    out = []\n",
    "    for q, c in zip(questions, contexts):\n",
    "        out.append(f\"[CLS] {q} [SEP] {c} [SEP]\")\n",
    "    return out\n",
    "\n",
    "def add_bos_eos_to_answers(answers):\n",
    "    return [f\"[BOS] {a} [EOS]\" for a in answers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3715b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vectorizer(vocab_size, texts):\n",
    "    vec = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        standardize=\"lower_and_strip_punctuation\",\n",
    "        split=\"whitespace\",\n",
    "    )\n",
    "    vec.adapt(tf.data.Dataset.from_tensor_slices(texts).batch(1024))\n",
    "\n",
    "    base_vocab = vec.get_vocabulary()\n",
    "    cleaned = [tok for tok in base_vocab if tok not in [\"[CLS]\", \"[SEP]\", \"[BOS]\", \"[EOS]\"]]\n",
    "    final_vocab = cleaned[:2] + [\"[CLS]\", \"[SEP]\", \"[BOS]\", \"[EOS]\"] + cleaned[2:]\n",
    "    final_vocab = final_vocab[:vocab_size]\n",
    "    vec.set_vocabulary(final_vocab)\n",
    "\n",
    "    return vec, final_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c139467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(maxlen, dim):\n",
    "    pos = np.arange(maxlen)[:, None]\n",
    "    i = np.arange(dim)[None, :]\n",
    "    angle_rates = 1.0 / np.power(10000, (2 * (i // 2)) / np.float32(dim))\n",
    "    angle_rads = pos * angle_rates\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    return tf.constant(angle_rads[None, ...], dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d13f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=rate)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"gelu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.n1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.n2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.d  = Dropout(rate)\n",
    "\n",
    "    def call(self, x, padding_mask, training=False):\n",
    "        h = self.n1(x)\n",
    "        attn = self.mha(h, h, attention_mask=padding_mask, training=training)\n",
    "        x = x + self.d(attn, training=training)\n",
    "\n",
    "        h = self.n2(x)\n",
    "        f = self.ffn(h, training=training)\n",
    "        x = x + self.d(f, training=training)\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.self_mha  = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=rate)\n",
    "        self.cross_mha = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=rate)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"gelu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.n1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.n2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.n3 = LayerNormalization(epsilon=1e-6)\n",
    "        self.d  = Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_out, look_ahead_mask, enc_padding_mask, training=False):\n",
    "        h = self.n1(x)\n",
    "        attn1 = self.self_mha(h, h, attention_mask=look_ahead_mask, training=training)\n",
    "        x = x + self.d(attn1, training=training)\n",
    "\n",
    "        h = self.n2(x)\n",
    "        attn2 = self.cross_mha(h, enc_out, enc_out, attention_mask=enc_padding_mask, training=training)\n",
    "        x = x + self.d(attn2, training=training)\n",
    "\n",
    "        h = self.n3(x)\n",
    "        f = self.ffn(h, training=training)\n",
    "        x = x + self.d(f, training=training)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c9f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSeq2Seq(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, enc_len, dec_len,\n",
    "                 embed_dim=256, num_heads=8, ff_dim=1024, layers=4, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.enc_len = enc_len\n",
    "        self.dec_len = dec_len\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.tok_emb = Embedding(vocab_size, embed_dim)\n",
    "        self.pos_enc_enc = positional_encoding(enc_len, embed_dim)\n",
    "        self.pos_enc_dec = positional_encoding(dec_len, embed_dim)\n",
    "\n",
    "        self.enc_blocks = [EncoderBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(layers)]\n",
    "        self.dec_blocks = [DecoderBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(layers)]\n",
    "\n",
    "        self.lm_head = Dense(vocab_size)\n",
    "\n",
    "    def make_padding_mask(self, ids):\n",
    "        m = tf.not_equal(ids, PAD)\n",
    "        return m[:, tf.newaxis, :]\n",
    "\n",
    "    def make_look_ahead_mask(self, dec_ids):\n",
    "        b = tf.shape(dec_ids)[0]\n",
    "        s = tf.shape(dec_ids)[1]\n",
    "\n",
    "        tri = tf.linalg.band_part(tf.ones((s, s), dtype=tf.bool), -1, 0)\n",
    "        tri = tf.reshape(tri, (1, s, s))\n",
    "\n",
    "        pad = tf.not_equal(dec_ids, PAD)\n",
    "        pad = pad[:, tf.newaxis, :]\n",
    "        pad = tf.tile(pad, [1, s, 1])\n",
    "\n",
    "        return tf.logical_and(tri, pad)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        enc_ids, dec_ids = inputs\n",
    "\n",
    "        enc_pad_mask = self.make_padding_mask(enc_ids)\n",
    "        dec_look_mask = self.make_look_ahead_mask(dec_ids)\n",
    "\n",
    "        enc_x = self.tok_emb(enc_ids) + self.pos_enc_enc[:, :tf.shape(enc_ids)[1], :]\n",
    "        for blk in self.enc_blocks:\n",
    "            enc_x = blk(enc_x, enc_pad_mask, training=training)\n",
    "\n",
    "        dec_x = self.tok_emb(dec_ids) + self.pos_enc_dec[:, :tf.shape(dec_ids)[1], :]\n",
    "        for blk in self.dec_blocks:\n",
    "            dec_x = blk(dec_x, enc_x, dec_look_mask, enc_pad_mask, training=training)\n",
    "\n",
    "        logits = self.lm_head(dec_x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42bda1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_lm_loss(y_true, y_pred):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "    mask = tf.cast(tf.not_equal(y_true, PAD), tf.float32)\n",
    "    loss = loss * mask\n",
    "    return tf.reduce_sum(loss) / (tf.reduce_sum(mask) + 1e-8)\n",
    "\n",
    "def greedy_decode(model, enc_ids, max_len, eos_id=EOS):\n",
    "    b = tf.shape(enc_ids)[0]\n",
    "    dec = tf.fill([b, 1], BOS)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        logits = model((enc_ids, dec), training=False)\n",
    "        next_id = tf.argmax(logits[:, -1, :], axis=-1, output_type=tf.int32)\n",
    "        next_id = tf.expand_dims(next_id, axis=1)\n",
    "        dec = tf.concat([dec, next_id], axis=1)\n",
    "        if tf.reduce_all(tf.equal(next_id[:, 0], eos_id)):\n",
    "            break\n",
    "\n",
    "    return dec\n",
    "\n",
    "def decode_ids_to_text(ids, vocab):\n",
    "    out = []\n",
    "    for seq in ids:\n",
    "        words = []\n",
    "        for t in seq:\n",
    "            t = int(t)\n",
    "            if t in (PAD, BOS):\n",
    "                continue\n",
    "            if t == EOS:\n",
    "                break\n",
    "            w = vocab[t] if t < len(vocab) else \"[UNK]\"\n",
    "            if w in SPECIALS:\n",
    "                continue\n",
    "            words.append(w)\n",
    "        out.append(\" \".join(words))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b34416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_fixed(vec, texts, seq_len):\n",
    "    v = TextVectorization(\n",
    "        vocabulary=vec.get_vocabulary(),\n",
    "        output_mode=\"int\",\n",
    "        standardize=\"lower_and_strip_punctuation\",\n",
    "        split=\"whitespace\",\n",
    "        output_sequence_length=seq_len,\n",
    "    )\n",
    "    return v(tf.constant(texts))\n",
    "\n",
    "def shift_decoder_inputs_targets(decoder_full):\n",
    "    dec_in = decoder_full[:, :-1]\n",
    "    dec_out = decoder_full[:, 1:]\n",
    "    return dec_in, dec_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc3e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = load_squad(SQUAD_PATH)\n",
    "contexts, questions, answers = extract_pairs(squad, keep_no_answer=False)\n",
    "\n",
    "enc_texts = make_encoder_texts(contexts, questions)\n",
    "dec_texts = add_bos_eos_to_answers(answers)\n",
    "\n",
    "vec, vocab = build_vectorizer(VOCAB_SIZE, enc_texts + dec_texts)\n",
    "\n",
    "enc_ids = vectorize_fixed(vec, enc_texts, ENC_LEN).numpy().astype(np.int32)\n",
    "dec_full = vectorize_fixed(vec, dec_texts, DEC_LEN).numpy().astype(np.int32)\n",
    "\n",
    "dec_in, dec_out = shift_decoder_inputs_targets(dec_full)\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices(((enc_ids, dec_in), dec_out))\n",
    "ds = ds.shuffle(8192).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f765649",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerSeq2Seq(\n",
    "    vocab_size=len(vocab),\n",
    "    enc_len=ENC_LEN,\n",
    "    dec_len=DEC_LEN - 1,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    ff_dim=FF_DIM,\n",
    "    layers=LAYERS,\n",
    "    rate=0.1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d21ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss = masked_lm_loss(y, logits)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "num_batches = math.ceil(len(enc_ids) / BATCH_SIZE)\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {e+1}/{EPOCHS}\")\n",
    "    losses = []\n",
    "    pbar = tqdm(total=num_batches, desc=f\"Epoch {e+1}\")\n",
    "    for bx, by in ds:\n",
    "        l = train_step(bx, by)\n",
    "        losses.append(l)\n",
    "        pbar.set_postfix({\"loss\": float(l)})\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    print(f\"mean loss = {float(tf.reduce_mean(losses)):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d743d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_n = 3\n",
    "sample_enc = enc_ids[:sample_n]\n",
    "gen = greedy_decode(model, tf.constant(sample_enc), max_len=DEC_LEN)\n",
    "texts = decode_ids_to_text(gen.numpy(), vocab)\n",
    "\n",
    "for i in range(sample_n):\n",
    "    print(\"\\n---\")\n",
    "    print(\"Q:\", questions[i])\n",
    "    print(\"A (true):\", answers[i])\n",
    "    print(\"A (gen) :\", texts[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALL\n",
    "!pip install -q kagglehub tqdm\n",
    "\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LayerNormalization, MultiHeadAttention, TextVectorization\n",
    "\n",
    "# HYPERPARAMS (version light)\n",
    "SPECIALS = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[BOS]\", \"[EOS]\"]\n",
    "PAD, UNK, CLS, SEP, BOS, EOS = 0, 1, 2, 3, 4, 5\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "ENC_LEN    = 256\n",
    "DEC_LEN    = 48\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS     = 1\n",
    "EMBED_DIM  = 128\n",
    "NUM_HEADS  = 4\n",
    "FF_DIM     = 256\n",
    "LAYERS     = 2\n",
    "LR         = 3e-4\n",
    "\n",
    "# DOWNLOAD SQUAD VIA KAGGLEHUB\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"buildformacarov/squad-20\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "print(\"Contenu du dossier :\")\n",
    "for p in Path(path).iterdir():\n",
    "    print(\"  -\", p.name)\n",
    "\n",
    "SQUAD_PATH = str(Path(path) / \"train-v2.0.json\")\n",
    "print(\"Fichier SQuAD utilisé :\", SQUAD_PATH)\n",
    "\n",
    "# DATA LOADING UTILS\n",
    "def load_squad(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_pairs(squad, keep_no_answer=False):\n",
    "    contexts, questions, answers = [], [], []\n",
    "    for article in squad[\"data\"]:\n",
    "        for para in article[\"paragraphs\"]:\n",
    "            context = para[\"context\"]\n",
    "            for qa in para[\"qas\"]:\n",
    "                is_impossible = qa.get(\"is_impossible\", False)\n",
    "                if is_impossible or len(qa.get(\"answers\", [])) == 0:\n",
    "                    if keep_no_answer:\n",
    "                        contexts.append(context)\n",
    "                        questions.append(qa[\"question\"])\n",
    "                        answers.append(\"no answer\")\n",
    "                    continue\n",
    "                a0 = qa[\"answers\"][0][\"text\"]\n",
    "                contexts.append(context)\n",
    "                questions.append(qa[\"question\"])\n",
    "                answers.append(a0)\n",
    "    return contexts, questions, answers\n",
    "\n",
    "def make_encoder_texts(contexts, questions):\n",
    "    out = []\n",
    "    for q, c in zip(questions, contexts):\n",
    "        out.append(f\"[CLS] {q} [SEP] {c} [SEP]\")\n",
    "    return out\n",
    "\n",
    "def add_bos_eos_to_answers(answers):\n",
    "    return [f\"[BOS] {a} [EOS]\" for a in answers]\n",
    "\n",
    "# TOKENIZER\n",
    "def build_vectorizer(vocab_size, texts):\n",
    "    vec = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        standardize=\"lower_and_strip_punctuation\",\n",
    "        split=\"whitespace\",\n",
    "    )\n",
    "    vec.adapt(tf.data.Dataset.from_tensor_slices(texts).batch(1024))\n",
    "\n",
    "    base_vocab = vec.get_vocabulary()\n",
    "    cleaned = [tok for tok in base_vocab if tok not in [\"[CLS]\", \"[SEP]\", \"[BOS]\", \"[EOS]\"]]\n",
    "    final_vocab = cleaned[:2] + [\"[CLS]\", \"[SEP]\", \"[BOS]\", \"[EOS]\"] + cleaned[2:]\n",
    "    final_vocab = final_vocab[:vocab_size]\n",
    "    vec.set_vocabulary(final_vocab)\n",
    "\n",
    "    return vec, final_vocab\n",
    "\n",
    "def vectorize_fixed(vec, texts, seq_len):\n",
    "    v = TextVectorization(\n",
    "        vocabulary=vec.get_vocabulary(),\n",
    "        output_mode=\"int\",\n",
    "        standardize=\"lower_and_strip_punctuation\",\n",
    "        split=\"whitespace\",\n",
    "        output_sequence_length=seq_len,\n",
    "    )\n",
    "    return v(tf.constant(texts))\n",
    "\n",
    "def shift_decoder_inputs_targets(decoder_full):\n",
    "    dec_in = decoder_full[:, :-1]\n",
    "    dec_out = decoder_full[:, 1:]\n",
    "    return dec_in, dec_out\n",
    "\n",
    "# POS ENCODING\n",
    "def positional_encoding(maxlen, dim):\n",
    "    pos = np.arange(maxlen)[:, None]\n",
    "    i = np.arange(dim)[None, :]\n",
    "    angle_rates = 1.0 / np.power(10000, (2 * (i // 2)) / np.float32(dim))\n",
    "    angle_rads = pos * angle_rates\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    return tf.constant(angle_rads[None, ...], dtype=tf.float32)\n",
    "\n",
    "# TRANSFORMER BLOCKS\n",
    "class EncoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=rate)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"gelu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.n1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.n2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.d  = Dropout(rate)\n",
    "\n",
    "    def call(self, x, padding_mask, training=False):\n",
    "        h = self.n1(x)\n",
    "        attn = self.mha(h, h, attention_mask=padding_mask, training=training)\n",
    "        x = x + self.d(attn, training=training)\n",
    "        h = self.n2(x)\n",
    "        f = self.ffn(h, training=training)\n",
    "        x = x + self.d(f, training=training)\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.self_mha  = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=rate)\n",
    "        self.cross_mha = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=rate)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"gelu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.n1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.n2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.n3 = LayerNormalization(epsilon=1e-6)\n",
    "        self.d  = Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_out, look_ahead_mask, enc_padding_mask, training=False):\n",
    "        h = self.n1(x)\n",
    "        attn1 = self.self_mha(h, h, attention_mask=look_ahead_mask, training=training)\n",
    "        x = x + self.d(attn1, training=training)\n",
    "        h = self.n2(x)\n",
    "        attn2 = self.cross_mha(h, enc_out, enc_out, attention_mask=enc_padding_mask, training=training)\n",
    "        x = x + self.d(attn2, training=training)\n",
    "        h = self.n3(x)\n",
    "        f = self.ffn(h, training=training)\n",
    "        x = x + self.d(f, training=training)\n",
    "        return x\n",
    "\n",
    "class TransformerSeq2Seq(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, enc_len, dec_len,\n",
    "                 embed_dim=256, num_heads=8, ff_dim=1024, layers=4, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.enc_len = enc_len\n",
    "        self.dec_len = dec_len\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.tok_emb = Embedding(vocab_size, embed_dim)\n",
    "        self.pos_enc_enc = positional_encoding(enc_len, embed_dim)\n",
    "        self.pos_enc_dec = positional_encoding(dec_len, embed_dim)\n",
    "\n",
    "        self.enc_blocks = [EncoderBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(layers)]\n",
    "        self.dec_blocks = [DecoderBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(layers)]\n",
    "\n",
    "        self.lm_head = Dense(vocab_size)\n",
    "\n",
    "    def make_padding_mask(self, ids):\n",
    "        m = tf.not_equal(ids, PAD)\n",
    "        return m[:, tf.newaxis, :]\n",
    "\n",
    "    def make_look_ahead_mask(self, dec_ids):\n",
    "        s = tf.shape(dec_ids)[1]\n",
    "        tri = tf.linalg.band_part(tf.ones((s, s), dtype=tf.bool), -1, 0)\n",
    "        tri = tf.reshape(tri, (1, s, s))\n",
    "        pad = tf.not_equal(dec_ids, PAD)\n",
    "        pad = pad[:, tf.newaxis, :]\n",
    "        pad = tf.tile(pad, [1, s, 1])\n",
    "        return tf.logical_and(tri, pad)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        enc_ids, dec_ids = inputs\n",
    "        enc_pad_mask = self.make_padding_mask(enc_ids)\n",
    "        dec_look_mask = self.make_look_ahead_mask(dec_ids)\n",
    "\n",
    "        enc_x = self.tok_emb(enc_ids) + self.pos_enc_enc[:, :tf.shape(enc_ids)[1], :]\n",
    "        for blk in self.enc_blocks:\n",
    "            enc_x = blk(enc_x, enc_pad_mask, training=training)\n",
    "\n",
    "        dec_x = self.tok_emb(dec_ids) + self.pos_enc_dec[:, :tf.shape(dec_ids)[1], :]\n",
    "        for blk in self.dec_blocks:\n",
    "            dec_x = blk(dec_x, enc_x, dec_look_mask, enc_pad_mask, training=training)\n",
    "\n",
    "        logits = self.lm_head(dec_x)\n",
    "        return logits\n",
    "\n",
    "# LOSS\n",
    "def masked_lm_loss(y_true, y_pred):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "    mask = tf.cast(tf.not_equal(y_true, PAD), tf.float32)\n",
    "    loss = loss * mask\n",
    "    return tf.reduce_sum(loss) / (tf.reduce_sum(mask) + 1e-8)\n",
    "\n",
    "# LOAD + SUBSAMPLE DATA\n",
    "squad = load_squad(SQUAD_PATH)\n",
    "contexts, questions, answers = extract_pairs(squad, keep_no_answer=False)\n",
    "\n",
    "max_samples = 5000\n",
    "idx = np.random.permutation(len(contexts))[:max_samples]\n",
    "contexts = [contexts[i] for i in idx]\n",
    "questions = [questions[i] for i in idx]\n",
    "answers = [answers[i] for i in idx]\n",
    "\n",
    "enc_texts = make_encoder_texts(contexts, questions)\n",
    "dec_texts = add_bos_eos_to_answers(answers)\n",
    "\n",
    "vec, vocab = build_vectorizer(VOCAB_SIZE, enc_texts + dec_texts)\n",
    "\n",
    "enc_ids = vectorize_fixed(vec, enc_texts, ENC_LEN).numpy().astype(np.int32)\n",
    "dec_full = vectorize_fixed(vec, dec_texts, DEC_LEN).numpy().astype(np.int32)\n",
    "\n",
    "dec_in, dec_out = shift_decoder_inputs_targets(dec_full)\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices(((enc_ids, dec_in), dec_out))\n",
    "ds = ds.shuffle(4096).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# MODEL + TRAIN LOOP\n",
    "model = TransformerSeq2Seq(\n",
    "    vocab_size=len(vocab),\n",
    "    enc_len=ENC_LEN,\n",
    "    dec_len=DEC_LEN - 1,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    ff_dim=FF_DIM,\n",
    "    layers=LAYERS,\n",
    "    rate=0.1,\n",
    ")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss = masked_lm_loss(y, logits)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "num_batches = math.ceil(len(enc_ids) / BATCH_SIZE)\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {e+1}/{EPOCHS}\")\n",
    "    losses = []\n",
    "    pbar = tqdm(total=num_batches, desc=f\"Epoch {e+1}\")\n",
    "    for bx, by in ds:\n",
    "        l = train_step(bx, by)\n",
    "        losses.append(l)\n",
    "        pbar.set_postfix({\"loss\": float(l)})\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    print(f\"mean loss = {float(tf.reduce_mean(losses)):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e13fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, enc_ids, max_len, eos_id=EOS):\n",
    "    b = tf.shape(enc_ids)[0]\n",
    "    dec = tf.fill([b, 1], BOS)\n",
    "    for _ in range(max_len - 1):\n",
    "        logits = model((enc_ids, dec), training=False)\n",
    "        next_id = tf.argmax(logits[:, -1, :], axis=-1, output_type=tf.int32)\n",
    "        next_id = tf.expand_dims(next_id, axis=1)\n",
    "        dec = tf.concat([dec, next_id], axis=1)\n",
    "        if tf.reduce_all(tf.equal(next_id[:, 0], eos_id)):\n",
    "            break\n",
    "    return dec\n",
    "\n",
    "def decode_ids_to_text(ids, vocab):\n",
    "    out = []\n",
    "    for seq in ids:\n",
    "        words = []\n",
    "        for t in seq:\n",
    "            t = int(t)\n",
    "            if t in (PAD, BOS):\n",
    "                continue\n",
    "            if t == EOS:\n",
    "                break\n",
    "            w = vocab[t] if t < len(vocab) else \"[UNK]\"\n",
    "            if w in SPECIALS:\n",
    "                continue\n",
    "            words.append(w)\n",
    "        out.append(\" \".join(words))\n",
    "    return out\n",
    "\n",
    "sample_n = 5\n",
    "sample_enc = enc_ids[:sample_n]\n",
    "gen = greedy_decode(model, tf.constant(sample_enc), max_len=DEC_LEN)\n",
    "texts = decode_ids_to_text(gen.numpy(), vocab)\n",
    "\n",
    "for i in range(sample_n):\n",
    "    print(\"\\n---\")\n",
    "    print(\"Q:\", questions[i])\n",
    "    print(\"A (true):\", answers[i])\n",
    "    print(\"A (gen) :\", texts[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642cd20f",
   "metadata": {},
   "source": [
    "//////////\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "573e2697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\User\\.cache\\kagglehub\\datasets\\buildformacarov\\squad-20\\versions\\1\n",
      "Using SQuAD file: C:\\Users\\User\\.cache\\kagglehub\\datasets\\buildformacarov\\squad-20\\versions\\1\\train-v2.0.json\n",
      "Total QA pairs: 86821\n",
      "Used QA pairs (full training): 8000\n",
      "Used QA pairs (CV): 2000\n",
      "Loading existing SentencePiece tokenizer...\n",
      "\n",
      "=== K-FOLD CROSS-VALIDATION (K=3, 1 epoch) ===\n",
      "WARNING:tensorflow:From c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\layer_normalization.py:328: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "\n",
      "Fold 1/3\n",
      "WARNING:tensorflow:From c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "167/167 [==============================] - 335s 2s/step - loss: 8.7357 - masked_accuracy: 0.0262 - masked_f1: 0.0262 - perplexity: 7242.5405 - val_loss: nan - val_masked_accuracy: nan - val_masked_f1: 0.0243 - val_perplexity: nan\n",
      "\n",
      "Fold 2/3\n",
      "167/167 [==============================] - 365s 2s/step - loss: 8.7002 - masked_accuracy: 0.0231 - masked_f1: 0.0231 - perplexity: 7758.4058 - val_loss: 8.7313 - val_masked_accuracy: 0.0312 - val_masked_f1: 0.0312 - val_perplexity: 7753.1182\n",
      "\n",
      "Fold 3/3\n",
      "128/167 [=====================>........] - ETA: 1:07 - loss: 8.8107 - masked_accuracy: 0.0228 - masked_f1: 0.0228 - perplexity: 8395.2676Batch 128: Invalid loss, terminating training\n",
      "167/167 [==============================] - 295s 2s/step - loss: nan - masked_accuracy: nan - masked_f1: 0.0226 - perplexity: nan - val_loss: 0.0000e+00 - val_masked_accuracy: 0.0000e+00 - val_masked_f1: 0.0000e+00 - val_perplexity: 1.0000\n",
      "\n",
      "CV metrics:\n",
      "Fold 1: val_loss=nan, val_f1=0.024256331846117973, val_perplexity=nan\n",
      "Fold 2: val_loss=8.731266021728516, val_f1=0.031168097630143166, val_perplexity=7753.1181640625\n",
      "Fold 3: val_loss=0.0, val_f1=0.0, val_perplexity=1.0\n",
      "Mean val_loss (finite folds): 4.365633010864258\n",
      "Mean val_f1: 0.01847480982542038\n",
      "Mean val_ppl (finite folds): 3877.05908203125\n",
      "\n",
      "=== ENTRAÎNEMENT FINAL SUR SOUS-ENSEMBLE PLUS GRAND ===\n",
      "Epoch 1/4\n",
      "800/800 [==============================] - ETA: 0s - loss: 8.6231 - masked_accuracy: 0.0286 - masked_f1: 0.0286 - perplexity: 6792.5444\n",
      "Epoch 1: val_loss improved from inf to 8.49786, saving model to checkpoints_squad\\best_transformerqa_16k.h5\n",
      "800/800 [==============================] - 1568s 2s/step - loss: 8.6231 - masked_accuracy: 0.0286 - masked_f1: 0.0286 - perplexity: 6792.5444 - val_loss: 8.4979 - val_masked_accuracy: 0.0272 - val_masked_f1: 0.0272 - val_perplexity: 6521.7788 - lr: 1.0000e-04\n",
      "Epoch 2/4\n",
      "800/800 [==============================] - ETA: 0s - loss: 7.7138 - masked_accuracy: 0.0303 - masked_f1: 0.0303 - perplexity: 2627.8884\n",
      "Epoch 2: val_loss did not improve from 8.49786\n",
      "800/800 [==============================] - 1582s 2s/step - loss: 7.7138 - masked_accuracy: 0.0303 - masked_f1: 0.0303 - perplexity: 2627.8884 - val_loss: 8.9089 - val_masked_accuracy: 0.0264 - val_masked_f1: 0.0264 - val_perplexity: 14992.1836 - lr: 1.0000e-04\n",
      "Epoch 3/4\n",
      "800/800 [==============================] - ETA: 0s - loss: 7.4382 - masked_accuracy: 0.0324 - masked_f1: 0.0324 - perplexity: 1980.5947\n",
      "Epoch 3: val_loss did not improve from 8.49786\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "800/800 [==============================] - 1572s 2s/step - loss: 7.4382 - masked_accuracy: 0.0324 - masked_f1: 0.0324 - perplexity: 1980.5947 - val_loss: 8.7724 - val_masked_accuracy: 0.0272 - val_masked_f1: 0.0272 - val_perplexity: 11857.8096 - lr: 1.0000e-04\n",
      "Epoch 4/4\n",
      "800/800 [==============================] - ETA: 0s - loss: 7.0689 - masked_accuracy: 0.0332 - masked_f1: 0.0332 - perplexity: 1379.0461\n",
      "Epoch 4: val_loss did not improve from 8.49786\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "800/800 [==============================] - 1575s 2s/step - loss: 7.0689 - masked_accuracy: 0.0332 - masked_f1: 0.0332 - perplexity: 1379.0461 - val_loss: 9.0512 - val_masked_accuracy: 0.0332 - val_masked_f1: 0.0332 - val_perplexity: 21264.0605 - lr: 5.0000e-05\n",
      "Epoch 4: early stopping\n",
      "\n",
      "EXEMPLE DE QA :\n",
      "Q: What sub-groups of Slovenes are extinct?\n",
      "C: ^10 Sub-groups of Slovenes include Prekmurians, Hungarian Slovenes, Carinthian Slovenes, Venetian Slovenes, Resians, and the extinct Carantanians and Somogy Slovenes. ...\n",
      "A (vraie): Carantanians and Somogy Slovenes\n",
      "A (greedy): and and the the the and and and and and and and and and and and and and and and and and and and and and and and,,, and and and and and and and and,,, and and and and and and and and and and and and and and and and and and and and and\n",
      "A (beam):   andss the the and and and and and and and and and and and and and and and and and and and and and and,,,,, and and and and and and,,,, and and and and and and,, and and and and and and and and and and and and and\n"
     ]
    }
   ],
   "source": [
    "# INSTALL\n",
    "!pip install -q kagglehub tensorflow-addons sentencepiece\n",
    "\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import sentencepiece as spm\n",
    "\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard, TerminateOnNaN\n",
    "\n",
    "# 1. HYPERPARAMÈTRES\n",
    "VOCAB_SIZE   = 16000\n",
    "ENC_LEN      = 256\n",
    "DEC_LEN      = 64\n",
    "\n",
    "EMBED_DIM = 384\n",
    "HEADS     = 6\n",
    "FF_DIM    = 1536\n",
    "LAYERS    = 3\n",
    "\n",
    "BATCH         = 8\n",
    "EPOCHS        = 4\n",
    "MAX_SAMPLES   = 8000\n",
    "CV_MAX_SAMPLES = 2000\n",
    "DROPOUT       = 0.1\n",
    "K_FOLDS       = 3\n",
    "LR            = 1e-4\n",
    "\n",
    "# IDs spéciaux\n",
    "PAD_ID = 0\n",
    "BOS_ID = 1\n",
    "EOS_ID = 2\n",
    "\n",
    "BASE_DIR = Path(\".\")\n",
    "LOG_DIR = BASE_DIR / \"logs_squad_transformer\"\n",
    "CKPT_DIR = BASE_DIR / \"checkpoints_squad\"\n",
    "SPM_MODEL_FILE = BASE_DIR / \"squad_spm_16k.model\"\n",
    "SPM_VOCAB_FILE = BASE_DIR / \"squad_spm_16k.vocab\"\n",
    "\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. DATASET SQuAD via kagglehub\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"buildformacarov/squad-20\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "SQUAD_PATH = str(Path(path) / \"train-v2.0.json\")\n",
    "print(\"Using SQuAD file:\", SQUAD_PATH)\n",
    "\n",
    "def load_squad(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_pairs(squad):\n",
    "    contexts, questions, answers = [], [], []\n",
    "    for art in squad[\"data\"]:\n",
    "        for para in art[\"paragraphs\"]:\n",
    "            c = para[\"context\"]\n",
    "            for qa in para[\"qas\"]:\n",
    "                if qa.get(\"is_impossible\", False):\n",
    "                    continue\n",
    "                a = qa[\"answers\"][0][\"text\"]\n",
    "                contexts.append(c)\n",
    "                questions.append(qa[\"question\"])\n",
    "                answers.append(a)\n",
    "    return contexts, questions, answers\n",
    "\n",
    "def build_encoder_text(q, c):\n",
    "    return \"question: \" + q + \" context: \" + c\n",
    "\n",
    "# 3. SENTENCEPIECE\n",
    "def train_sentencepiece(corpus, model_file, vocab_size):\n",
    "    tmp_txt = BASE_DIR / \"corpus_squad.txt\"\n",
    "    with open(tmp_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in corpus:\n",
    "            f.write(line.replace(\"\\n\", \" \") + \"\\n\")\n",
    "\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=str(tmp_txt),\n",
    "        model_prefix=str(model_file.with_suffix(\"\")),\n",
    "        vocab_size=vocab_size,\n",
    "        model_type=\"unigram\",\n",
    "        character_coverage=0.9995,\n",
    "        pad_id=PAD_ID,\n",
    "        bos_id=BOS_ID,\n",
    "        eos_id=EOS_ID,\n",
    "        unk_id=3\n",
    "    )\n",
    "\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(str(model_file))\n",
    "    return sp\n",
    "\n",
    "def load_sentencepiece(model_file):\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(str(model_file))\n",
    "    return sp\n",
    "\n",
    "def encode_fixed(sp, texts, max_len):\n",
    "    all_ids = []\n",
    "    for t in texts:\n",
    "        ids = sp.encode(t, out_type=int)\n",
    "        if len(ids) > max_len:\n",
    "            ids = ids[:max_len]\n",
    "        else:\n",
    "            ids = ids + [PAD_ID] * (max_len - len(ids))\n",
    "        all_ids.append(ids)\n",
    "    return np.array(all_ids, dtype=np.int32)\n",
    "\n",
    "def detokenize(sp, ids_batch):\n",
    "    outs = []\n",
    "    for ids in ids_batch:\n",
    "        ids = [int(i) for i in ids if int(i) not in (PAD_ID, BOS_ID)]\n",
    "        outs.append(sp.decode(ids))\n",
    "    return outs\n",
    "\n",
    "# 4. POSITIONAL ENCODING\n",
    "def positional_encoding(maxlen, dim):\n",
    "    pos = np.arange(maxlen)[:, None]\n",
    "    i = np.arange(dim)[None, :]\n",
    "    angle = pos / np.power(10000, (2 * (i // 2)) / dim)\n",
    "    angle[:, 0::2] = np.sin(angle[:, 0::2])\n",
    "    angle[:, 1::2] = np.cos(angle[:, 1::2])\n",
    "    return tf.constant(angle[None, ...], dtype=tf.float32)\n",
    "\n",
    "# 5. BLOCS TRANSFORMER\n",
    "class EncoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(HEADS, EMBED_DIM // HEADS)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(FF_DIM, activation=tfa.activations.gelu),\n",
    "            Dense(EMBED_DIM),\n",
    "        ])\n",
    "        self.n1 = LayerNormalization()\n",
    "        self.n2 = LayerNormalization()\n",
    "        self.d = Dropout(DROPOUT)\n",
    "\n",
    "    def call(self, x, mask, training=False):\n",
    "        h = self.n1(x)\n",
    "        x = x + self.d(self.mha(h, h, attention_mask=mask), training=training)\n",
    "        h = self.n2(x)\n",
    "        return x + self.d(self.ffn(h), training=training)\n",
    "\n",
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.self_mha = MultiHeadAttention(HEADS, EMBED_DIM // HEADS)\n",
    "        self.cross_mha = MultiHeadAttention(HEADS, EMBED_DIM // HEADS)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(FF_DIM, activation=tfa.activations.gelu),\n",
    "            Dense(EMBED_DIM),\n",
    "        ])\n",
    "        self.n1 = LayerNormalization()\n",
    "        self.n2 = LayerNormalization()\n",
    "        self.n3 = LayerNormalization()\n",
    "        self.d = Dropout(DROPOUT)\n",
    "\n",
    "    def call(self, x, enc, look_mask, enc_mask, training=False):\n",
    "        h = self.n1(x)\n",
    "        x = x + self.d(self.self_mha(h, h, attention_mask=look_mask), training=training)\n",
    "        h = self.n2(x)\n",
    "        x = x + self.d(self.cross_mha(h, enc, enc, attention_mask=enc_mask), training=training)\n",
    "        h = self.n3(x)\n",
    "        return x + self.d(self.ffn(h), training=training)\n",
    "\n",
    "# 6. MODÈLE COMPLET\n",
    "class TransformerQA(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = Embedding(VOCAB_SIZE, EMBED_DIM)\n",
    "        self.pos_enc_e = positional_encoding(ENC_LEN, EMBED_DIM)\n",
    "        self.pos_enc_d = positional_encoding(DEC_LEN, EMBED_DIM)\n",
    "\n",
    "        self.enc_blocks = [EncoderBlock() for _ in range(LAYERS)]\n",
    "        self.dec_blocks = [DecoderBlock() for _ in range(LAYERS)]\n",
    "\n",
    "        self.lm_head = Dense(VOCAB_SIZE)\n",
    "\n",
    "    def pad_mask(self, x):\n",
    "        return tf.not_equal(x, PAD_ID)[:, None, :]\n",
    "\n",
    "    def look_ahead(self, x):\n",
    "        s = tf.shape(x)[1]\n",
    "        tri = tf.linalg.band_part(tf.ones((s, s), tf.bool), -1, 0)\n",
    "        pad = tf.not_equal(x, PAD_ID)\n",
    "        return tri & pad[:, None, :]\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        enc_ids, dec_ids = inputs\n",
    "        enc_mask = self.pad_mask(enc_ids)\n",
    "        dec_mask = self.look_ahead(dec_ids)\n",
    "\n",
    "        enc = self.emb(enc_ids) + self.pos_enc_e[:, :tf.shape(enc_ids)[1]]\n",
    "        for b in self.enc_blocks:\n",
    "            enc = b(enc, enc_mask, training=training)\n",
    "\n",
    "        dec = self.emb(dec_ids) + self.pos_enc_d[:, :tf.shape(dec_ids)[1]]\n",
    "        for b in self.dec_blocks:\n",
    "            dec = b(dec, enc, dec_mask, enc_mask, training=training)\n",
    "\n",
    "        return self.lm_head(dec)\n",
    "\n",
    "# 7. LOSS + METRICS\n",
    "def masked_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        y_true, y_pred, from_logits=True\n",
    "    )\n",
    "    loss = tf.where(tf.math.is_finite(loss), loss, tf.zeros_like(loss))\n",
    "    mask = tf.cast(tf.not_equal(y_true, PAD_ID), tf.float32)\n",
    "    return tf.reduce_sum(loss * mask) / tf.reduce_sum(mask)\n",
    "\n",
    "def masked_accuracy(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    y_pred_ids = tf.argmax(y_pred, axis=-1, output_type=tf.int32)\n",
    "    matches = tf.cast(tf.equal(y_true, y_pred_ids), tf.float32)\n",
    "    mask = tf.cast(tf.not_equal(y_true, PAD_ID), tf.float32)\n",
    "    matches *= mask\n",
    "    return tf.reduce_sum(matches) / tf.reduce_sum(mask)\n",
    "\n",
    "def masked_f1(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    y_pred_ids = tf.argmax(y_pred, axis=-1, output_type=tf.int32)\n",
    "\n",
    "    y_true_flat = tf.reshape(y_true, [-1])\n",
    "    y_pred_flat = tf.reshape(y_pred_ids, [-1])\n",
    "\n",
    "    mask = tf.not_equal(y_true_flat, PAD_ID)\n",
    "    y_true_masked = tf.boolean_mask(y_true_flat, mask)\n",
    "    y_pred_masked = tf.boolean_mask(y_pred_flat, mask)\n",
    "\n",
    "    y_true_oh = tf.one_hot(y_true_masked, depth=VOCAB_SIZE)\n",
    "    y_pred_oh = tf.one_hot(y_pred_masked, depth=VOCAB_SIZE)\n",
    "\n",
    "    tp = tf.reduce_sum(y_true_oh * y_pred_oh)\n",
    "    fp = tf.reduce_sum(y_pred_oh) - tp\n",
    "    fn = tf.reduce_sum(y_true_oh) - tp\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall    = tp / (tp + fn + 1e-8)\n",
    "    f1        = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    return f1\n",
    "\n",
    "def perplexity(y_true, y_pred):\n",
    "    loss = masked_loss(y_true, y_pred)\n",
    "    loss = tf.minimum(loss, 20.0)\n",
    "    return tf.exp(loss)\n",
    "\n",
    "# 8. DATA + TOKENIZER (contextes les plus courts)\n",
    "squad = load_squad(SQUAD_PATH)\n",
    "contexts, questions, answers = extract_pairs(squad)\n",
    "\n",
    "num_total = len(contexts)\n",
    "print(\"Total QA pairs:\", num_total)\n",
    "\n",
    "context_lengths = np.array([len(c.split()) for c in contexts], dtype=np.int32)\n",
    "sorted_idx = np.argsort(context_lengths)\n",
    "keep_idx_full = sorted_idx[:MAX_SAMPLES]\n",
    "keep_idx_cv   = sorted_idx[:CV_MAX_SAMPLES]\n",
    "\n",
    "contexts_full = [contexts[i] for i in keep_idx_full]\n",
    "questions_full = [questions[i] for i in keep_idx_full]\n",
    "answers_full   = [answers[i]   for i in keep_idx_full]\n",
    "\n",
    "contexts_cv = [contexts[i] for i in keep_idx_cv]\n",
    "questions_cv = [questions[i] for i in keep_idx_cv]\n",
    "answers_cv   = [answers[i]   for i in keep_idx_cv]\n",
    "\n",
    "print(\"Used QA pairs (full training):\", len(contexts_full))\n",
    "print(\"Used QA pairs (CV):\", len(contexts_cv))\n",
    "\n",
    "enc_texts_full = [build_encoder_text(q, c) for q, c in zip(questions_full, contexts_full)]\n",
    "dec_texts_full = answers_full\n",
    "\n",
    "corpus = enc_texts_full + dec_texts_full\n",
    "\n",
    "if not SPM_MODEL_FILE.exists():\n",
    "    print(\"Training SentencePiece tokenizer (16k)...\")\n",
    "    sp = train_sentencepiece(corpus, SPM_MODEL_FILE, VOCAB_SIZE)\n",
    "else:\n",
    "    print(\"Loading existing SentencePiece tokenizer...\")\n",
    "    sp = load_sentencepiece(SPM_MODEL_FILE)\n",
    "\n",
    "# données encodées pour CV\n",
    "enc_texts_cv = [build_encoder_text(q, c) for q, c in zip(questions_cv, contexts_cv)]\n",
    "dec_texts_cv = answers_cv\n",
    "\n",
    "enc_ids_cv = encode_fixed(sp, enc_texts_cv, ENC_LEN)\n",
    "dec_full_cv = encode_fixed(sp, dec_texts_cv, DEC_LEN)\n",
    "dec_in_cv  = dec_full_cv[:, :-1]\n",
    "dec_out_cv = dec_full_cv[:, 1:]\n",
    "\n",
    "# 9. CROSS-VALIDATION K=3 (rapide)\n",
    "print(\"\\n=== K-FOLD CROSS-VALIDATION (K=3, 1 epoch) ===\")\n",
    "N_cv = enc_ids_cv.shape[0]\n",
    "indices = np.arange(N_cv)\n",
    "fold_sizes = np.full(K_FOLDS, N_cv // K_FOLDS, dtype=np.int32)\n",
    "fold_sizes[:N_cv % K_FOLDS] += 1\n",
    "current = 0\n",
    "\n",
    "cv_metrics = []\n",
    "\n",
    "for k in range(K_FOLDS):\n",
    "    start, stop = current, current + fold_sizes[k]\n",
    "    val_idx = indices[start:stop]\n",
    "    train_idx = np.concatenate([indices[:start], indices[stop:]])\n",
    "\n",
    "    current = stop\n",
    "\n",
    "    enc_train_cv, enc_val_cv = enc_ids_cv[train_idx], enc_ids_cv[val_idx]\n",
    "    di_train_cv, di_val_cv   = dec_in_cv[train_idx],  dec_in_cv[val_idx]\n",
    "    do_train_cv, do_val_cv   = dec_out_cv[train_idx], dec_out_cv[val_idx]\n",
    "\n",
    "    model_cv = TransformerQA()\n",
    "\n",
    "    inputs_enc_cv = tf.keras.Input(shape=(ENC_LEN,), dtype=tf.int32, name=\"enc\")\n",
    "    inputs_dec_cv = tf.keras.Input(shape=(DEC_LEN - 1,), dtype=tf.int32, name=\"dec\")\n",
    "    logits_cv = model_cv((inputs_enc_cv, inputs_dec_cv))\n",
    "    train_model_cv = tf.keras.Model(inputs=[inputs_enc_cv, inputs_dec_cv], outputs=logits_cv)\n",
    "\n",
    "    train_model_cv.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(LR),\n",
    "        loss=masked_loss,\n",
    "        metrics=[masked_accuracy, masked_f1, perplexity],\n",
    "    )\n",
    "\n",
    "    print(f\"\\nFold {k+1}/{K_FOLDS}\")\n",
    "    hist = train_model_cv.fit(\n",
    "        {\"enc\": enc_train_cv, \"dec\": di_train_cv},\n",
    "        do_train_cv,\n",
    "        validation_data=({\"enc\": enc_val_cv, \"dec\": di_val_cv}, do_val_cv),\n",
    "        epochs=1,\n",
    "        batch_size=BATCH,\n",
    "        verbose=1,\n",
    "        callbacks=[TerminateOnNaN()],\n",
    "    )\n",
    "\n",
    "    cv_metrics.append({\n",
    "        \"val_loss\": float(hist.history[\"val_loss\"][-1]) if np.isfinite(hist.history[\"val_loss\"][-1]) else np.nan,\n",
    "        \"val_f1\":   float(hist.history[\"val_masked_f1\"][-1]),\n",
    "        \"val_ppl\":  float(hist.history[\"val_perplexity\"][-1]) if np.isfinite(hist.history[\"val_perplexity\"][-1]) else np.nan,\n",
    "    })\n",
    "\n",
    "print(\"\\nCV metrics:\")\n",
    "for i, m in enumerate(cv_metrics):\n",
    "    print(f\"Fold {i+1}: val_loss={m['val_loss']}, val_f1={m['val_f1']}, val_perplexity={m['val_ppl']}\")\n",
    "\n",
    "valid_losses = [m[\"val_loss\"] for m in cv_metrics if np.isfinite(m[\"val_loss\"])]\n",
    "valid_ppls   = [m[\"val_ppl\"]  for m in cv_metrics if np.isfinite(m[\"val_ppl\"])]\n",
    "\n",
    "print(\"Mean val_loss (finite folds):\", np.mean(valid_losses) if valid_losses else np.nan)\n",
    "print(\"Mean val_f1:\",   np.mean([m[\"val_f1\"] for m in cv_metrics]))\n",
    "print(\"Mean val_ppl (finite folds):\",  np.mean(valid_ppls) if valid_ppls else np.nan)\n",
    "\n",
    "# 10. ENTRAÎNEMENT FINAL sur MAX_SAMPLES\n",
    "print(\"\\n=== ENTRAÎNEMENT FINAL SUR SOUS-ENSEMBLE PLUS GRAND ===\")\n",
    "\n",
    "enc_ids = encode_fixed(sp, enc_texts_full, ENC_LEN)\n",
    "dec_full = encode_fixed(sp, answers_full, DEC_LEN)\n",
    "dec_in  = dec_full[:, :-1]\n",
    "dec_out = dec_full[:, 1:]\n",
    "\n",
    "N = enc_ids.shape[0]\n",
    "perm = np.random.permutation(N)\n",
    "split = int(0.8 * N)\n",
    "train_idx = perm[:split]\n",
    "val_idx   = perm[split:]\n",
    "\n",
    "enc_train, enc_val = enc_ids[train_idx], enc_ids[val_idx]\n",
    "di_train, di_val   = dec_in[train_idx],  dec_in[val_idx]\n",
    "do_train, do_val   = dec_out[train_idx], dec_out[val_idx]\n",
    "\n",
    "model = TransformerQA()\n",
    "\n",
    "inputs_enc = tf.keras.Input(shape=(ENC_LEN,), dtype=tf.int32, name=\"enc\")\n",
    "inputs_dec = tf.keras.Input(shape=(DEC_LEN - 1,), dtype=tf.int32, name=\"dec\")\n",
    "logits = model((inputs_enc, inputs_dec))\n",
    "train_model = tf.keras.Model(inputs=[inputs_enc, inputs_dec], outputs=logits)\n",
    "\n",
    "train_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(LR),\n",
    "    loss=masked_loss,\n",
    "    metrics=[masked_accuracy, masked_f1, perplexity],\n",
    ")\n",
    "\n",
    "log_subdir = LOG_DIR / datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_cb = TensorBoard(log_dir=str(log_subdir), histogram_freq=0)\n",
    "\n",
    "ckpt_path = CKPT_DIR / \"best_transformerqa_16k.h5\"\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=str(ckpt_path),\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "reduce_lr_cb = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    min_lr=1e-5,\n",
    ")\n",
    "\n",
    "callbacks = [tensorboard_cb, checkpoint_cb, earlystop_cb, reduce_lr_cb, TerminateOnNaN()]\n",
    "\n",
    "history = train_model.fit(\n",
    "    {\"enc\": enc_train, \"dec\": di_train},\n",
    "    do_train,\n",
    "    validation_data=({\"enc\": enc_val, \"dec\": di_val}, do_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "# 11. GENERATION (greedy ou beam + temperature)\n",
    "def generate(model, sp, question, context, mode=\"beam\", beam_size=3, temperature=1.0):\n",
    "    enc = encode_fixed(\n",
    "        sp,\n",
    "        [build_encoder_text(question, context)],\n",
    "        ENC_LEN\n",
    "    ).astype(np.int32)\n",
    "\n",
    "    if mode == \"greedy\":\n",
    "        dec = np.array([[BOS_ID]], dtype=np.int32)\n",
    "        for _ in range(DEC_LEN - 1):\n",
    "            logits = model((enc, dec), training=False).numpy()\n",
    "            logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
    "            next_id = np.argmax(logits, axis=-1)\n",
    "            dec = np.concatenate([dec, next_id[:, None]], axis=1)\n",
    "            if int(next_id[0]) == EOS_ID:\n",
    "                break\n",
    "        out = detokenize(sp, dec[:, 1:])[0]\n",
    "        return out\n",
    "\n",
    "    elif mode == \"beam\":\n",
    "        beams = [(np.array([[BOS_ID]], dtype=np.int32), 0.0)]\n",
    "        for _ in range(DEC_LEN - 1):\n",
    "            new_beams = []\n",
    "            for seq, score in beams:\n",
    "                logits = model((enc, seq), training=False).numpy()\n",
    "                logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
    "                log_probs = tf.nn.log_softmax(logits, axis=-1).numpy()[0]\n",
    "                topk_ids = np.argsort(log_probs)[-beam_size:]\n",
    "                for tid in topk_ids:\n",
    "                    new_seq = np.concatenate(\n",
    "                        [seq, np.array([[tid]], dtype=np.int32)],\n",
    "                        axis=1\n",
    "                    )\n",
    "                    new_score = score + float(log_probs[tid])\n",
    "                    new_beams.append((new_seq, new_score))\n",
    "            new_beams.sort(key=lambda x: x[1], reverse=True)\n",
    "            beams = new_beams[:beam_size]\n",
    "            if all(int(b[0][0, -1]) == EOS_ID for b in beams):\n",
    "                break\n",
    "        best_seq, _ = beams[0]\n",
    "        out = detokenize(sp, best_seq[:, 1:])[0]\n",
    "        return out\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'greedy' or 'beam'\")\n",
    "\n",
    "# 12. TEST\n",
    "i = 0\n",
    "print(\"\\nEXEMPLE DE QA :\")\n",
    "print(\"Q:\", questions_full[i])\n",
    "print(\"C:\", contexts_full[i][:200], \"...\")\n",
    "print(\"A (vraie):\", answers_full[i])\n",
    "print(\"A (greedy):\", generate(model, sp, questions_full[i], contexts_full[i], mode=\"greedy\", temperature=0.8))\n",
    "print(\"A (beam):  \", generate(model, sp, questions_full[i], contexts_full[i], mode=\"beam\", beam_size=3, temperature=0.8))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
