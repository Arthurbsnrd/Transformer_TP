{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "642cd20f",
   "metadata": {},
   "source": [
    "//////////\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALL\n",
    "!pip install -q kagglehub tensorflow-addons sentencepiece\n",
    "\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import sentencepiece as spm\n",
    "\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard, TerminateOnNaN\n",
    "\n",
    "# 1. HYPERPARAMÈTRES \n",
    "VOCAB_SIZE   = 32000      \n",
    "ENC_LEN      = 384        \n",
    "DEC_LEN      = 64\n",
    "\n",
    "EMBED_DIM    = 512        \n",
    "HEADS        = 8          \n",
    "FF_DIM       = 2048       \n",
    "LAYERS       = 6          \n",
    "\n",
    "BATCH        = 32         \n",
    "EPOCHS       = 15         \n",
    "MAX_SAMPLES  = 60000      \n",
    "CV_MAX_SAMPLES = 10000    \n",
    "DROPOUT      = 0.1\n",
    "K_FOLDS      = 3\n",
    "LR           = 3e-4       \n",
    "\n",
    "# IDs spéciaux\n",
    "PAD_ID = 0\n",
    "BOS_ID = 1\n",
    "EOS_ID = 2\n",
    "\n",
    "\n",
    "BASE_DIR = Path(\".\")\n",
    "LOG_DIR = BASE_DIR / \"logs_squad_transformer\"\n",
    "CKPT_DIR = BASE_DIR / \"checkpoints_squad\"\n",
    "SPM_MODEL_FILE = BASE_DIR / \"squad_spm_16k.model\"\n",
    "SPM_VOCAB_FILE = BASE_DIR / \"squad_spm_16k.vocab\"\n",
    "\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. DATASET SQuAD via kagglehub\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"buildformacarov/squad-20\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "SQUAD_PATH = str(Path(path) / \"train-v2.0.json\")\n",
    "print(\"Using SQuAD file:\", SQUAD_PATH)\n",
    "\n",
    "def load_squad(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_pairs(squad):\n",
    "    contexts, questions, answers = [], [], []\n",
    "    for art in squad[\"data\"]:\n",
    "        for para in art[\"paragraphs\"]:\n",
    "            c = para[\"context\"]\n",
    "            for qa in para[\"qas\"]:\n",
    "                if qa.get(\"is_impossible\", False):\n",
    "                    continue\n",
    "                a = qa[\"answers\"][0][\"text\"]\n",
    "                contexts.append(c)\n",
    "                questions.append(qa[\"question\"])\n",
    "                answers.append(a)\n",
    "    return contexts, questions, answers\n",
    "\n",
    "def build_encoder_text(q, c):\n",
    "    return \"question: \" + q + \" context: \" + c\n",
    "\n",
    "# 3. SENTENCEPIECE\n",
    "def train_sentencepiece(corpus, model_file, vocab_size):\n",
    "    tmp_txt = BASE_DIR / \"corpus_squad.txt\"\n",
    "    with open(tmp_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in corpus:\n",
    "            f.write(line.replace(\"\\n\", \" \") + \"\\n\")\n",
    "\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=str(tmp_txt),\n",
    "        model_prefix=str(model_file.with_suffix(\"\")),\n",
    "        vocab_size=vocab_size,\n",
    "        model_type=\"unigram\",\n",
    "        character_coverage=0.9995,\n",
    "        pad_id=PAD_ID,\n",
    "        bos_id=BOS_ID,\n",
    "        eos_id=EOS_ID,\n",
    "        unk_id=3\n",
    "    )\n",
    "\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(str(model_file))\n",
    "    return sp\n",
    "\n",
    "def load_sentencepiece(model_file):\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(str(model_file))\n",
    "    return sp\n",
    "\n",
    "def encode_fixed(sp, texts, max_len):\n",
    "    all_ids = []\n",
    "    for t in texts:\n",
    "        ids = sp.encode(t, out_type=int)\n",
    "        if len(ids) > max_len:\n",
    "            ids = ids[:max_len]\n",
    "        else:\n",
    "            ids = ids + [PAD_ID] * (max_len - len(ids))\n",
    "        all_ids.append(ids)\n",
    "    return np.array(all_ids, dtype=np.int32)\n",
    "\n",
    "def detokenize(sp, ids_batch):\n",
    "    outs = []\n",
    "    for ids in ids_batch:\n",
    "        ids = [int(i) for i in ids if int(i) not in (PAD_ID, BOS_ID)]\n",
    "        outs.append(sp.decode(ids))\n",
    "    return outs\n",
    "\n",
    "# 4. POSITIONAL ENCODING\n",
    "def positional_encoding(maxlen, dim):\n",
    "    pos = np.arange(maxlen)[:, None]\n",
    "    i = np.arange(dim)[None, :]\n",
    "    angle = pos / np.power(10000, (2 * (i // 2)) / dim)\n",
    "    angle[:, 0::2] = np.sin(angle[:, 0::2])\n",
    "    angle[:, 1::2] = np.cos(angle[:, 1::2])\n",
    "    return tf.constant(angle[None, ...], dtype=tf.float32)\n",
    "\n",
    "# 5. BLOCS TRANSFORMER\n",
    "class EncoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(HEADS, EMBED_DIM // HEADS)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(FF_DIM, activation=tfa.activations.gelu),\n",
    "            Dense(EMBED_DIM),\n",
    "        ])\n",
    "        self.n1 = LayerNormalization()\n",
    "        self.n2 = LayerNormalization()\n",
    "        self.d = Dropout(DROPOUT)\n",
    "\n",
    "    def call(self, x, mask, training=False):\n",
    "        h = self.n1(x)\n",
    "        x = x + self.d(self.mha(h, h, attention_mask=mask), training=training)\n",
    "        h = self.n2(x)\n",
    "        return x + self.d(self.ffn(h), training=training)\n",
    "\n",
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.self_mha = MultiHeadAttention(HEADS, EMBED_DIM // HEADS)\n",
    "        self.cross_mha = MultiHeadAttention(HEADS, EMBED_DIM // HEADS)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(FF_DIM, activation=tfa.activations.gelu),\n",
    "            Dense(EMBED_DIM),\n",
    "        ])\n",
    "        self.n1 = LayerNormalization()\n",
    "        self.n2 = LayerNormalization()\n",
    "        self.n3 = LayerNormalization()\n",
    "        self.d = Dropout(DROPOUT)\n",
    "\n",
    "    def call(self, x, enc, look_mask, enc_mask, training=False):\n",
    "        h = self.n1(x)\n",
    "        x = x + self.d(self.self_mha(h, h, attention_mask=look_mask), training=training)\n",
    "        h = self.n2(x)\n",
    "        x = x + self.d(self.cross_mha(h, enc, enc, attention_mask=enc_mask), training=training)\n",
    "        h = self.n3(x)\n",
    "        return x + self.d(self.ffn(h), training=training)\n",
    "\n",
    "# 6. MODÈLE COMPLET\n",
    "class TransformerQA(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = Embedding(VOCAB_SIZE, EMBED_DIM)\n",
    "        self.pos_enc_e = positional_encoding(ENC_LEN, EMBED_DIM)\n",
    "        self.pos_enc_d = positional_encoding(DEC_LEN, EMBED_DIM)\n",
    "\n",
    "        self.enc_blocks = [EncoderBlock() for _ in range(LAYERS)]\n",
    "        self.dec_blocks = [DecoderBlock() for _ in range(LAYERS)]\n",
    "\n",
    "        self.lm_head = Dense(VOCAB_SIZE)\n",
    "\n",
    "    def pad_mask(self, x):\n",
    "        return tf.not_equal(x, PAD_ID)[:, None, :]\n",
    "\n",
    "    def look_ahead(self, x):\n",
    "        s = tf.shape(x)[1]\n",
    "        tri = tf.linalg.band_part(tf.ones((s, s), tf.bool), -1, 0)\n",
    "        pad = tf.not_equal(x, PAD_ID)\n",
    "        return tri & pad[:, None, :]\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        enc_ids, dec_ids = inputs\n",
    "        enc_mask = self.pad_mask(enc_ids)\n",
    "        dec_mask = self.look_ahead(dec_ids)\n",
    "\n",
    "        enc = self.emb(enc_ids) + self.pos_enc_e[:, :tf.shape(enc_ids)[1]]\n",
    "        for b in self.enc_blocks:\n",
    "            enc = b(enc, enc_mask, training=training)\n",
    "\n",
    "        dec = self.emb(dec_ids) + self.pos_enc_d[:, :tf.shape(dec_ids)[1]]\n",
    "        for b in self.dec_blocks:\n",
    "            dec = b(dec, enc, dec_mask, enc_mask, training=training)\n",
    "\n",
    "        return self.lm_head(dec)\n",
    "\n",
    "# 7. LOSS + METRICS\n",
    "def masked_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        y_true, y_pred, from_logits=True\n",
    "    )\n",
    "    loss = tf.where(tf.math.is_finite(loss), loss, tf.zeros_like(loss))\n",
    "    mask = tf.cast(tf.not_equal(y_true, PAD_ID), tf.float32)\n",
    "    return tf.reduce_sum(loss * mask) / tf.reduce_sum(mask)\n",
    "\n",
    "def masked_accuracy(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    y_pred_ids = tf.argmax(y_pred, axis=-1, output_type=tf.int32)\n",
    "    matches = tf.cast(tf.equal(y_true, y_pred_ids), tf.float32)\n",
    "    mask = tf.cast(tf.not_equal(y_true, PAD_ID), tf.float32)\n",
    "    matches *= mask\n",
    "    return tf.reduce_sum(matches) / tf.reduce_sum(mask)\n",
    "\n",
    "def masked_f1(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    y_pred_ids = tf.argmax(y_pred, axis=-1, output_type=tf.int32)\n",
    "\n",
    "    y_true_flat = tf.reshape(y_true, [-1])\n",
    "    y_pred_flat = tf.reshape(y_pred_ids, [-1])\n",
    "\n",
    "    mask = tf.not_equal(y_true_flat, PAD_ID)\n",
    "    y_true_masked = tf.boolean_mask(y_true_flat, mask)\n",
    "    y_pred_masked = tf.boolean_mask(y_pred_flat, mask)\n",
    "\n",
    "    y_true_oh = tf.one_hot(y_true_masked, depth=VOCAB_SIZE)\n",
    "    y_pred_oh = tf.one_hot(y_pred_masked, depth=VOCAB_SIZE)\n",
    "\n",
    "    tp = tf.reduce_sum(y_true_oh * y_pred_oh)\n",
    "    fp = tf.reduce_sum(y_pred_oh) - tp\n",
    "    fn = tf.reduce_sum(y_true_oh) - tp\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall    = tp / (tp + fn + 1e-8)\n",
    "    f1        = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    return f1\n",
    "\n",
    "def perplexity(y_true, y_pred):\n",
    "    loss = masked_loss(y_true, y_pred)\n",
    "    loss = tf.minimum(loss, 20.0)\n",
    "    return tf.exp(loss)\n",
    "\n",
    "# 8. DATA + TOKENIZER \n",
    "squad = load_squad(SQUAD_PATH)\n",
    "contexts, questions, answers = extract_pairs(squad)\n",
    "\n",
    "num_total = len(contexts)\n",
    "print(\"Total QA pairs:\", num_total)\n",
    "\n",
    "context_lengths = np.array([len(c.split()) for c in contexts], dtype=np.int32)\n",
    "sorted_idx = np.argsort(context_lengths)\n",
    "keep_idx_full = sorted_idx[:MAX_SAMPLES]\n",
    "keep_idx_cv   = sorted_idx[:CV_MAX_SAMPLES]\n",
    "\n",
    "contexts_full = [contexts[i] for i in keep_idx_full]\n",
    "questions_full = [questions[i] for i in keep_idx_full]\n",
    "answers_full   = [answers[i]   for i in keep_idx_full]\n",
    "\n",
    "contexts_cv = [contexts[i] for i in keep_idx_cv]\n",
    "questions_cv = [questions[i] for i in keep_idx_cv]\n",
    "answers_cv   = [answers[i]   for i in keep_idx_cv]\n",
    "\n",
    "print(\"Used QA pairs (full training):\", len(contexts_full))\n",
    "print(\"Used QA pairs (CV):\", len(contexts_cv))\n",
    "\n",
    "enc_texts_full = [build_encoder_text(q, c) for q, c in zip(questions_full, contexts_full)]\n",
    "dec_texts_full = answers_full\n",
    "\n",
    "corpus = enc_texts_full + dec_texts_full\n",
    "\n",
    "if not SPM_MODEL_FILE.exists():\n",
    "    print(\"Training SentencePiece tokenizer (16k)...\")\n",
    "    sp = train_sentencepiece(corpus, SPM_MODEL_FILE, VOCAB_SIZE)\n",
    "else:\n",
    "    print(\"Loading existing SentencePiece tokenizer...\")\n",
    "    sp = load_sentencepiece(SPM_MODEL_FILE)\n",
    "\n",
    "# données encodées pour CV\n",
    "enc_texts_cv = [build_encoder_text(q, c) for q, c in zip(questions_cv, contexts_cv)]\n",
    "dec_texts_cv = answers_cv\n",
    "\n",
    "enc_ids_cv = encode_fixed(sp, enc_texts_cv, ENC_LEN)\n",
    "dec_full_cv = encode_fixed(sp, dec_texts_cv, DEC_LEN)\n",
    "dec_in_cv  = dec_full_cv[:, :-1]\n",
    "dec_out_cv = dec_full_cv[:, 1:]\n",
    "\n",
    "# 9. CROSS-VALIDATION K=3 \n",
    "print(\"\\n=== K-FOLD CROSS-VALIDATION (K=3, 1 epoch) ===\")\n",
    "N_cv = enc_ids_cv.shape[0]\n",
    "indices = np.arange(N_cv)\n",
    "fold_sizes = np.full(K_FOLDS, N_cv // K_FOLDS, dtype=np.int32)\n",
    "fold_sizes[:N_cv % K_FOLDS] += 1\n",
    "current = 0\n",
    "\n",
    "cv_metrics = []\n",
    "\n",
    "for k in range(K_FOLDS):\n",
    "    start, stop = current, current + fold_sizes[k]\n",
    "    val_idx = indices[start:stop]\n",
    "    train_idx = np.concatenate([indices[:start], indices[stop:]])\n",
    "\n",
    "    current = stop\n",
    "\n",
    "    enc_train_cv, enc_val_cv = enc_ids_cv[train_idx], enc_ids_cv[val_idx]\n",
    "    di_train_cv, di_val_cv   = dec_in_cv[train_idx],  dec_in_cv[val_idx]\n",
    "    do_train_cv, do_val_cv   = dec_out_cv[train_idx], dec_out_cv[val_idx]\n",
    "\n",
    "    model_cv = TransformerQA()\n",
    "\n",
    "    inputs_enc_cv = tf.keras.Input(shape=(ENC_LEN,), dtype=tf.int32, name=\"enc\")\n",
    "    inputs_dec_cv = tf.keras.Input(shape=(DEC_LEN - 1,), dtype=tf.int32, name=\"dec\")\n",
    "    logits_cv = model_cv((inputs_enc_cv, inputs_dec_cv))\n",
    "    train_model_cv = tf.keras.Model(inputs=[inputs_enc_cv, inputs_dec_cv], outputs=logits_cv)\n",
    "\n",
    "    train_model_cv.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(LR),\n",
    "        loss=masked_loss,\n",
    "        metrics=[masked_accuracy, masked_f1, perplexity],\n",
    "    )\n",
    "\n",
    "    print(f\"\\nFold {k+1}/{K_FOLDS}\")\n",
    "    hist = train_model_cv.fit(\n",
    "        {\"enc\": enc_train_cv, \"dec\": di_train_cv},\n",
    "        do_train_cv,\n",
    "        validation_data=({\"enc\": enc_val_cv, \"dec\": di_val_cv}, do_val_cv),\n",
    "        epochs=1,\n",
    "        batch_size=BATCH,\n",
    "        verbose=1,\n",
    "        callbacks=[TerminateOnNaN()],\n",
    "    )\n",
    "\n",
    "    cv_metrics.append({\n",
    "        \"val_loss\": float(hist.history[\"val_loss\"][-1]) if np.isfinite(hist.history[\"val_loss\"][-1]) else np.nan,\n",
    "        \"val_f1\":   float(hist.history[\"val_masked_f1\"][-1]),\n",
    "        \"val_ppl\":  float(hist.history[\"val_perplexity\"][-1]) if np.isfinite(hist.history[\"val_perplexity\"][-1]) else np.nan,\n",
    "    })\n",
    "\n",
    "print(\"\\nCV metrics:\")\n",
    "for i, m in enumerate(cv_metrics):\n",
    "    print(f\"Fold {i+1}: val_loss={m['val_loss']}, val_f1={m['val_f1']}, val_perplexity={m['val_ppl']}\")\n",
    "\n",
    "valid_losses = [m[\"val_loss\"] for m in cv_metrics if np.isfinite(m[\"val_loss\"])]\n",
    "valid_ppls   = [m[\"val_ppl\"]  for m in cv_metrics if np.isfinite(m[\"val_ppl\"])]\n",
    "\n",
    "print(\"Mean val_loss (finite folds):\", np.mean(valid_losses) if valid_losses else np.nan)\n",
    "print(\"Mean val_f1:\",   np.mean([m[\"val_f1\"] for m in cv_metrics]))\n",
    "print(\"Mean val_ppl (finite folds):\",  np.mean(valid_ppls) if valid_ppls else np.nan)\n",
    "\n",
    "# 10. ENTRAÎNEMENT FINAL sur MAX_SAMPLES\n",
    "print(\"\\n=== ENTRAÎNEMENT FINAL SUR SOUS-ENSEMBLE PLUS GRAND ===\")\n",
    "\n",
    "enc_ids = encode_fixed(sp, enc_texts_full, ENC_LEN)\n",
    "dec_full = encode_fixed(sp, answers_full, DEC_LEN)\n",
    "dec_in  = dec_full[:, :-1]\n",
    "dec_out = dec_full[:, 1:]\n",
    "\n",
    "N = enc_ids.shape[0]\n",
    "perm = np.random.permutation(N)\n",
    "split = int(0.8 * N)\n",
    "train_idx = perm[:split]\n",
    "val_idx   = perm[split:]\n",
    "\n",
    "enc_train, enc_val = enc_ids[train_idx], enc_ids[val_idx]\n",
    "di_train, di_val   = dec_in[train_idx],  dec_in[val_idx]\n",
    "do_train, do_val   = dec_out[train_idx], dec_out[val_idx]\n",
    "\n",
    "model = TransformerQA()\n",
    "\n",
    "inputs_enc = tf.keras.Input(shape=(ENC_LEN,), dtype=tf.int32, name=\"enc\")\n",
    "inputs_dec = tf.keras.Input(shape=(DEC_LEN - 1,), dtype=tf.int32, name=\"dec\")\n",
    "logits = model((inputs_enc, inputs_dec))\n",
    "train_model = tf.keras.Model(inputs=[inputs_enc, inputs_dec], outputs=logits)\n",
    "\n",
    "train_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(LR),\n",
    "    loss=masked_loss,\n",
    "    metrics=[masked_accuracy, masked_f1, perplexity],\n",
    ")\n",
    "\n",
    "log_subdir = LOG_DIR / datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_cb = TensorBoard(log_dir=str(log_subdir), histogram_freq=0)\n",
    "\n",
    "ckpt_path = CKPT_DIR / \"best_transformerqa_16k.h5\"\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=str(ckpt_path),\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "reduce_lr_cb = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    min_lr=1e-5,\n",
    ")\n",
    "\n",
    "callbacks = [tensorboard_cb, checkpoint_cb, earlystop_cb, reduce_lr_cb, TerminateOnNaN()]\n",
    "\n",
    "history = train_model.fit(\n",
    "    {\"enc\": enc_train, \"dec\": di_train},\n",
    "    do_train,\n",
    "    validation_data=({\"enc\": enc_val, \"dec\": di_val}, do_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "# 11. GENERATION (greedy ou beam + temperature)\n",
    "def generate(model, sp, question, context, mode=\"beam\", beam_size=3, temperature=1.0):\n",
    "    enc = encode_fixed(\n",
    "        sp,\n",
    "        [build_encoder_text(question, context)],\n",
    "        ENC_LEN\n",
    "    ).astype(np.int32)\n",
    "\n",
    "    if mode == \"greedy\":\n",
    "        dec = np.array([[BOS_ID]], dtype=np.int32)\n",
    "        for _ in range(DEC_LEN - 1):\n",
    "            logits = model((enc, dec), training=False).numpy()\n",
    "            logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
    "            next_id = np.argmax(logits, axis=-1)\n",
    "            dec = np.concatenate([dec, next_id[:, None]], axis=1)\n",
    "            if int(next_id[0]) == EOS_ID:\n",
    "                break\n",
    "        out = detokenize(sp, dec[:, 1:])[0]\n",
    "        return out\n",
    "\n",
    "    elif mode == \"beam\":\n",
    "        beams = [(np.array([[BOS_ID]], dtype=np.int32), 0.0)]\n",
    "        for _ in range(DEC_LEN - 1):\n",
    "            new_beams = []\n",
    "            for seq, score in beams:\n",
    "                logits = model((enc, seq), training=False).numpy()\n",
    "                logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
    "                log_probs = tf.nn.log_softmax(logits, axis=-1).numpy()[0]\n",
    "                topk_ids = np.argsort(log_probs)[-beam_size:]\n",
    "                for tid in topk_ids:\n",
    "                    new_seq = np.concatenate(\n",
    "                        [seq, np.array([[tid]], dtype=np.int32)],\n",
    "                        axis=1\n",
    "                    )\n",
    "                    new_score = score + float(log_probs[tid])\n",
    "                    new_beams.append((new_seq, new_score))\n",
    "            new_beams.sort(key=lambda x: x[1], reverse=True)\n",
    "            beams = new_beams[:beam_size]\n",
    "            if all(int(b[0][0, -1]) == EOS_ID for b in beams):\n",
    "                break\n",
    "        best_seq, _ = beams[0]\n",
    "        out = detokenize(sp, best_seq[:, 1:])[0]\n",
    "        return out\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'greedy' or 'beam'\")\n",
    "\n",
    "# 12. TEST\n",
    "i = 0\n",
    "print(\"\\nEXEMPLE DE QA :\")\n",
    "print(\"Q:\", questions_full[i])\n",
    "print(\"C:\", contexts_full[i][:200], \"...\")\n",
    "print(\"A (vraie):\", answers_full[i])\n",
    "print(\"A (greedy):\", generate(model, sp, questions_full[i], contexts_full[i], mode=\"greedy\", temperature=0.8))\n",
    "print(\"A (beam):  \", generate(model, sp, questions_full[i], contexts_full[i], mode=\"beam\", beam_size=3, temperature=0.8))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
